import os
import time

import pytest

from spb_onprem import DiagnosisService, DatasetService, ContentService, SliceService
from spb_onprem.diagnoses.entities import DiagnosisStatus
from spb_onprem.reports.entities.analytics_report_item import AnalyticsReportItemType
from spb_onprem.charts import (
    ChartDataFactory,
    CategoryValueData,
    HeatmapData,
    ConfusionMatrixData,
    LineChartData,
    ScatterPlotData,
    BinFrequencyData,
    MetricData,
    DataIdsIndex,
    XYDataIds,
    LineChartDataIds,
)


def test_diagnosis_lifecycle_workflow():
    """
    Complete diagnosis lifecycle workflow test:
    - Find a dataset automatically
    - Create a new diagnosis with all parameters including new fields
    - Get the created diagnosis by ID
    - Get the created diagnosis by name
    - Update diagnosis with various parameters including new fields
    - Test diagnosis listing and filtering
    - Create diagnosis report items with chart data
    - Upload chart data for all chart types
    - Update diagnosis report items
    - Delete diagnosis report items (optional)
    - Delete the diagnosis (optional)
    - Verify deletion
    """
    if os.environ.get("CI") == "true":
        pytest.skip("Skip workflow tests on CI")
    if os.environ.get("RUN_DIAGNOSIS_WORKFLOW_TESTS") != "1":
        pytest.skip("RUN_DIAGNOSIS_WORKFLOW_TESTS!=1 (avoid accidental mutations)")
    
    # Configuration flag
    CLEANUP = os.environ.get("CLEANUP", "1") == "1"
    
    diagnosis_service = DiagnosisService()
    dataset_service = DatasetService()
    slice_service = SliceService()
    content_service = ContentService()
    
    print("=" * 80)
    print("Diagnosis Service Complete Lifecycle Workflow Test")
    print("=" * 80)
    print(f"\n‚öôÔ∏è  Configuration:")
    print(f"   CLEANUP: {CLEANUP} (Delete diagnosis and all reports after test)")
    
    # ==================== FIND DATASET ====================
    
    print("\n[Step 0] Finding a dataset for diagnosis testing...")
    DATASET_ID = None
    
    try:
        # Get first available dataset
        datasets, _, total = dataset_service.get_datasets(length=1)
        
        if total > 0 and len(datasets) > 0:
            dataset = datasets[0]
            DATASET_ID = dataset.id
            print(f"‚úÖ Found dataset: {dataset.name} (ID: {dataset.id})")
            print(f"   Total datasets available: {total}")
        else:
            print("‚ùå No datasets found")
            print("‚ö†Ô∏è  Please create at least one dataset first")
            pytest.fail("No datasets found")
        
        print(f"‚úÖ Using dataset:")
        print(f"   Dataset ID: {DATASET_ID}")
            
    except Exception as e:
        print(f"‚ùå Failed to find dataset: {e}")
        print(f"‚ö†Ô∏è  Please check your dataset configuration")
        pytest.fail(str(e))
    
    # Get slices for source/target slice IDs
    try:
        slices, _, _ = slice_service.get_slices(dataset_id=DATASET_ID)
    except Exception as e:
        print(f"‚ùå Failed to get slices: {e}")
        slices = []
    
    print(f"\nüìã Test Configuration:")
    print(f"   Dataset ID: {DATASET_ID}")
    
    # Test diagnosis details
    test_diagnosis_name = f"workflow_test_diagnosis_{int(time.time())}"
    test_description = "Diagnosis created by workflow test"
    
    print(f"   Diagnosis Name: {test_diagnosis_name}")
    print(f"   Description: {test_description}")
    
    created_diagnosis_id = None
    created_diagnosis_report_ids = []
    
    try:
        # ==================== CREATE DIAGNOSIS ====================
        
        print("\n[Step 1] Creating a new diagnosis with detailed parameters...")
        
        # First, create some content files for the diagnosis
        print("   Creating sample content files for diagnosis...")
        diagnosis_contents = {}
        try:
            # Create config.yaml content
            config_content, _ = content_service.create_content(
                key="diagnosis_config.yaml",
                content_type="application/yaml"
            )
            diagnosis_contents["diagnosis_config.yaml"] = config_content.id
            print(f"   ‚úÖ Created diagnosis_config.yaml: {config_content.id}")
            
            # Create results.json content
            results_content, _ = content_service.create_content(
                key="diagnosis_results.json",
                content_type="application/json"
            )
            diagnosis_contents["diagnosis_results.json"] = results_content.id
            print(f"   ‚úÖ Created diagnosis_results.json: {results_content.id}")
            
            # Create log.txt content
            log_content, _ = content_service.create_content(
                key="diagnosis_log.txt",
                content_type="text/plain"
            )
            diagnosis_contents["diagnosis_log.txt"] = log_content.id
            print(f"   ‚úÖ Created diagnosis_log.txt: {log_content.id}")
            
        except Exception as content_error:
            print(f"   ‚ö†Ô∏è  Failed to create content files: {content_error}")
            diagnosis_contents = None
        
        try:
            # Create diagnosis with comprehensive parameters including new fields
            created_diagnosis = diagnosis_service.create_diagnosis(
                dataset_id=DATASET_ID,
                name=test_diagnosis_name,
                description=test_description,
                score_key="accuracy",
                score_value=0.0,
                score_unit="%",
                diagnosis_parameters={
                    "threshold": 0.5,
                    "min_confidence": 0.7,
                    "algorithm": "detection_analysis"
                },
                contents=diagnosis_contents,
                source_slice_id=slices[0].id if slices else None,
                target_slice_id=slices[1].id if len(slices) > 1 else None,
                source_data_count=5000,
                target_data_count=3000,
                diagnosis_data_count=2000
            )
            created_diagnosis_id = created_diagnosis.id
            
            print(f"‚úÖ Diagnosis created successfully")
            print(f"   Diagnosis ID: {created_diagnosis.id}")
            print(f"   Name: {created_diagnosis.name}")
            print(f"   Description: {created_diagnosis.description}")
            print(f"   Status: {created_diagnosis.status}")
            print(f"   Score Key: {created_diagnosis.score_key}")
            print(f"   Score Value: {created_diagnosis.score_value}")
            print(f"   Score Unit: {created_diagnosis.score_unit}")
            print(f"   Diagnosis Parameters: {created_diagnosis.diagnosis_parameters}")
            print(f"   Source Slice ID: {created_diagnosis.source_slice_id}")
            print(f"   Target Slice ID: {created_diagnosis.target_slice_id}")
            print(f"   Source Data Count: {created_diagnosis.source_data_count}")
            print(f"   Target Data Count: {created_diagnosis.target_data_count}")
            print(f"   Diagnosis Data Count: {created_diagnosis.diagnosis_data_count}")
            
            # Verify contents field
            if diagnosis_contents:
                print(f"   Contents: {len(diagnosis_contents)} files")
                for filename, content_id in diagnosis_contents.items():
                    print(f"      - {filename}: {content_id}")
                
                if created_diagnosis.contents:
                    print(f"   ‚úÖ Contents field verified in created diagnosis")
                    assert created_diagnosis.contents == diagnosis_contents, "Contents field mismatch"
                else:
                    print(f"   ‚ö†Ô∏è  Contents field is None in created diagnosis (API may not support it yet)")
            
            print(f"   Created At: {created_diagnosis.created_at}")
            print(f"   Created By: {created_diagnosis.created_by}")
            
            assert created_diagnosis.name == test_diagnosis_name, "Diagnosis name mismatch"
            assert created_diagnosis.description == test_description, "Description mismatch"
            assert created_diagnosis.score_key == "accuracy", "Score key mismatch"
            assert created_diagnosis.score_value == 0.0, "Score value mismatch"
            assert created_diagnosis.diagnosis_parameters is not None, "Diagnosis parameters should not be None"
            
            # Verify new fields
            if slices:
                assert created_diagnosis.source_slice_id is not None, "Source slice ID should be set"
                print(f"   ‚úÖ Source slice ID verified")
            assert created_diagnosis.source_data_count == 5000, "Source data count mismatch"
            assert created_diagnosis.target_data_count == 3000, "Target data count mismatch"
            assert created_diagnosis.diagnosis_data_count == 2000, "Diagnosis data count mismatch"
            print(f"   ‚úÖ All new fields verified")
            
        except Exception as e:
            print(f"‚ùå Failed to create diagnosis: {e}")
            pytest.fail(str(e))
        
        # ==================== GET DIAGNOSIS BY ID ====================
        
        print("\n[Step 2] Getting diagnosis by ID...")
        try:
            retrieved_diagnosis = diagnosis_service.get_diagnosis(
                dataset_id=DATASET_ID,
                diagnosis_id=created_diagnosis_id
            )
            
            print(f"‚úÖ Retrieved diagnosis by ID successfully")
            print(f"   Diagnosis ID: {retrieved_diagnosis.id}")
            print(f"   Name: {retrieved_diagnosis.name}")
            print(f"   Status: {retrieved_diagnosis.status}")
            
            # Verify contents field persists
            if diagnosis_contents:
                if retrieved_diagnosis.contents:
                    print(f"   ‚úÖ Contents field persists: {len(retrieved_diagnosis.contents)} files")
                    assert retrieved_diagnosis.contents == diagnosis_contents, "Contents field mismatch in retrieved diagnosis"
                else:
                    print(f"   ‚ö†Ô∏è  Contents field is None in retrieved diagnosis")
            
            # Verify new fields persist
            assert retrieved_diagnosis.source_data_count == 5000, "Source data count mismatch in retrieved diagnosis"
            assert retrieved_diagnosis.target_data_count == 3000, "Target data count mismatch in retrieved diagnosis"
            assert retrieved_diagnosis.diagnosis_data_count == 2000, "Diagnosis data count mismatch in retrieved diagnosis"
            print(f"   ‚úÖ All new fields persist")
            
            assert retrieved_diagnosis.id == created_diagnosis_id, "Retrieved diagnosis ID mismatch"
            assert retrieved_diagnosis.name == test_diagnosis_name, "Retrieved diagnosis name mismatch"
            
        except Exception as e:
            print(f"‚ùå Failed to get diagnosis by ID: {e}")
            pytest.fail(str(e))
        
        # ==================== GET DIAGNOSIS BY NAME ====================
        
        print("\n[Step 3] Getting diagnosis by name...")
        try:
            retrieved_by_name = diagnosis_service.get_diagnosis_by_name(
                dataset_id=DATASET_ID,
                name=test_diagnosis_name
            )
            
            print(f"‚úÖ Retrieved diagnosis by name successfully")
            print(f"   Diagnosis ID: {retrieved_by_name.id}")
            print(f"   Name: {retrieved_by_name.name}")
            print(f"   Status: {retrieved_by_name.status}")
            
            assert retrieved_by_name.id == created_diagnosis_id, "Diagnosis ID mismatch when retrieving by name"
            assert retrieved_by_name.name == test_diagnosis_name, "Diagnosis name mismatch"
            
        except Exception as e:
            print(f"‚ùå Failed to get diagnosis by name: {e}")
            pytest.fail(str(e))
        
        # ==================== UPDATE DIAGNOSIS - BASIC INFO ====================
        
        print("\n[Step 4] Updating diagnosis basic information...")
        try:
            updated_description = "Updated description for workflow test"
            updated_diagnosis = diagnosis_service.update_diagnosis(
                dataset_id=DATASET_ID,
                diagnosis_id=created_diagnosis_id,
                description=updated_description
            )
            
            print(f"‚úÖ Updated diagnosis basic information successfully")
            print(f"   Updated Description: {updated_diagnosis.description}")
            
            # Verify contents field still persists after update
            if diagnosis_contents:
                if updated_diagnosis.contents:
                    print(f"   ‚úÖ Contents field still persists after update: {len(updated_diagnosis.contents)} files")
                    assert updated_diagnosis.contents == diagnosis_contents, "Contents field changed unexpectedly"
                else:
                    print(f"   ‚ö†Ô∏è  Contents field is None after update")
            
            assert updated_diagnosis.description == updated_description, "Description was not updated"
            
        except Exception as e:
            print(f"‚ùå Failed to update diagnosis basic information: {e}")
            pytest.fail(str(e))
        
        # ==================== UPDATE DIAGNOSIS - STATUS ====================
        
        print("\n[Step 5] Updating diagnosis status...")
        try:
            updated_diagnosis_status = diagnosis_service.update_diagnosis(
                dataset_id=DATASET_ID,
                diagnosis_id=created_diagnosis_id,
                status=DiagnosisStatus.IN_PROGRESS
            )
            
            print(f"‚úÖ Updated diagnosis status successfully")
            print(f"   New Status: {updated_diagnosis_status.status}")
            
            assert updated_diagnosis_status.status == DiagnosisStatus.IN_PROGRESS, "Status was not updated"
            
        except Exception as e:
            print(f"‚ùå Failed to update diagnosis status: {e}")
            pytest.fail(str(e))
        
        # ==================== UPDATE DIAGNOSIS - DATA COUNTS ====================
        
        print("\n[Step 6] Updating diagnosis data counts...")
        try:
            updated_diagnosis_counts = diagnosis_service.update_diagnosis(
                dataset_id=DATASET_ID,
                diagnosis_id=created_diagnosis_id,
                source_data_count=6000,
                target_data_count=4000,
                diagnosis_data_count=2500
            )
            
            print(f"‚úÖ Updated diagnosis data counts successfully")
            print(f"   Source Data Count: {updated_diagnosis_counts.source_data_count}")
            print(f"   Target Data Count: {updated_diagnosis_counts.target_data_count}")
            print(f"   Diagnosis Data Count: {updated_diagnosis_counts.diagnosis_data_count}")
            
            assert updated_diagnosis_counts.source_data_count == 6000, "Source data count was not updated"
            assert updated_diagnosis_counts.target_data_count == 4000, "Target data count was not updated"
            assert updated_diagnosis_counts.diagnosis_data_count == 2500, "Diagnosis data count was not updated"
            
        except Exception as e:
            print(f"‚ùå Failed to update diagnosis data counts: {e}")
            pytest.fail(str(e))
        
        # ==================== UPDATE DIAGNOSIS - PARAMETERS ====================
        
        print("\n[Step 7] Updating diagnosis parameters...")
        try:
            new_diagnosis_params = {
                "threshold": 0.6,
                "min_confidence": 0.8,
                "algorithm": "improved_detection_analysis",
                "iterations": 100
            }
            
            updated_diagnosis_params = diagnosis_service.update_diagnosis(
                dataset_id=DATASET_ID,
                diagnosis_id=created_diagnosis_id,
                diagnosis_parameters=new_diagnosis_params
            )
            
            print(f"‚úÖ Updated diagnosis parameters successfully")
            print(f"   Diagnosis Parameters: {updated_diagnosis_params.diagnosis_parameters}")
            
            assert updated_diagnosis_params.diagnosis_parameters is not None, "Diagnosis parameters should not be None"
            assert updated_diagnosis_params.diagnosis_parameters.get("threshold") == 0.6, "Threshold was not updated"
            assert updated_diagnosis_params.diagnosis_parameters.get("iterations") == 100, "Iterations was not added"
            
        except Exception as e:
            print(f"‚ùå Failed to update diagnosis parameters: {e}")
            pytest.fail(str(e))
        
        # ==================== UPDATE DIAGNOSIS - CONTENTS ====================
        
        print("\n[Step 8] Updating diagnosis contents...")
        if diagnosis_contents:
            try:
                # Add a new file to contents
                analysis_content, _ = content_service.create_content(
                    key="detailed_analysis.json",
                    content_type="application/json"
                )
                print(f"   ‚úÖ Created detailed_analysis.json: {analysis_content.id}")
                
                updated_contents = {**diagnosis_contents, "detailed_analysis.json": analysis_content.id}
                
                updated_diagnosis_contents = diagnosis_service.update_diagnosis(
                    dataset_id=DATASET_ID,
                    diagnosis_id=created_diagnosis_id,
                    contents=updated_contents
                )
                
                print(f"‚úÖ Updated diagnosis contents successfully")
                
                if updated_diagnosis_contents.contents:
                    print(f"   Contents: {len(updated_diagnosis_contents.contents)} files")
                    for filename, content_id in updated_diagnosis_contents.contents.items():
                        print(f"      - {filename}: {content_id}")
                    
                    assert len(updated_diagnosis_contents.contents) == 4, "Should have 4 files now"
                    assert "detailed_analysis.json" in updated_diagnosis_contents.contents, "New file should be in contents"
                    assert updated_diagnosis_contents.contents["detailed_analysis.json"] == analysis_content.id, "New file content_id mismatch"
                    
                    # Update diagnosis_contents for subsequent checks
                    diagnosis_contents = updated_contents
                    print(f"   ‚úÖ Contents field verified after update")
                else:
                    print(f"   ‚ö†Ô∏è  Contents field is None after update")
                
            except Exception as e:
                print(f"‚ùå Failed to update diagnosis contents: {e}")
                pytest.fail(str(e))
        else:
            print("   ‚è≠Ô∏è  Skipping contents update (initial creation failed)")
        
        # ==================== UPDATE DIAGNOSIS - SLICE IDS ====================
        
        print("\n[Step 9] Updating diagnosis slice IDs...")
        if len(slices) >= 2:
            try:
                updated_diagnosis_slices = diagnosis_service.update_diagnosis(
                    dataset_id=DATASET_ID,
                    diagnosis_id=created_diagnosis_id,
                    source_slice_id=slices[1].id,
                    target_slice_id=slices[0].id
                )
                
                print(f"‚úÖ Updated diagnosis slice IDs successfully")
                print(f"   Source Slice ID: {updated_diagnosis_slices.source_slice_id}")
                print(f"   Target Slice ID: {updated_diagnosis_slices.target_slice_id}")
                
                assert updated_diagnosis_slices.source_slice_id == slices[1].id, "Source slice ID was not updated"
                assert updated_diagnosis_slices.target_slice_id == slices[0].id, "Target slice ID was not updated"
                
            except Exception as e:
                print(f"‚ùå Failed to update diagnosis slice IDs: {e}")
                pytest.fail(str(e))
        else:
            print("   ‚è≠Ô∏è  Skipping slice ID update (not enough slices)")
        
        # ==================== UPDATE DIAGNOSIS - SCORE ====================
        
        print("\n[Step 10] Updating diagnosis score...")
        try:
            updated_diagnosis_score = diagnosis_service.update_diagnosis(
                dataset_id=DATASET_ID,
                diagnosis_id=created_diagnosis_id,
                score_key="f1_score",
                score_value=0.92,
                score_unit="%"
            )
            
            print(f"‚úÖ Updated diagnosis score successfully")
            print(f"   Score Key: {updated_diagnosis_score.score_key}")
            print(f"   Score Value: {updated_diagnosis_score.score_value}")
            print(f"   Score Unit: {updated_diagnosis_score.score_unit}")
            
            assert updated_diagnosis_score.score_key == "f1_score", "Score key was not updated"
            assert updated_diagnosis_score.score_value == 0.92, "Score value was not updated"
            
        except Exception as e:
            print(f"‚ùå Failed to update diagnosis score: {e}")
            pytest.fail(str(e))
        
        # ==================== UPDATE DIAGNOSIS - COMPLETE STATUS ====================
        
        print("\n[Step 11] Marking diagnosis as completed...")
        try:
            completed_diagnosis = diagnosis_service.update_diagnosis(
                dataset_id=DATASET_ID,
                diagnosis_id=created_diagnosis_id,
                status=DiagnosisStatus.COMPLETED
            )
            
            print(f"‚úÖ Marked diagnosis as completed successfully")
            print(f"   Status: {completed_diagnosis.status}")
            print(f"   Completed At: {completed_diagnosis.completed_at}")
            
            assert completed_diagnosis.status == DiagnosisStatus.COMPLETED, "Status was not updated to COMPLETED"
            
        except Exception as e:
            print(f"‚ùå Failed to mark diagnosis as completed: {e}")
            pytest.fail(str(e))
        
        # ==================== LIST DIAGNOSES ====================
        
        print("\n[Step 12] Listing diagnoses in dataset...")
        try:
            diagnoses_list, next_cursor, total_count = diagnosis_service.get_diagnoses(
                dataset_id=DATASET_ID,
                length=10
            )
            
            print(f"‚úÖ Retrieved diagnoses list successfully")
            print(f"   Total Count: {total_count}")
            print(f"   Retrieved Count: {len(diagnoses_list)}")
            print(f"   Next Cursor: {next_cursor}")
            
            # Find our created diagnosis in the list
            found_diagnosis = None
            for diagnosis in diagnoses_list:
                if diagnosis.id == created_diagnosis_id:
                    found_diagnosis = diagnosis
                    break
            
            if found_diagnosis:
                print(f"   ‚úÖ Found our test diagnosis in the list:")
                print(f"      Diagnosis ID: {found_diagnosis.id}")
                print(f"      Name: {found_diagnosis.name}")
                print(f"      Status: {found_diagnosis.status}")
            else:
                print(f"   ‚ö†Ô∏è  Test diagnosis not found in first page (might be on next page)")
            
            assert total_count > 0, "Should have at least one diagnosis"
            assert len(diagnoses_list) > 0, "Should return at least one diagnosis"
            
        except Exception as e:
            print(f"‚ùå Failed to list diagnoses: {e}")
            pytest.fail(str(e))
        
        # ==================== CREATE DIAGNOSIS REPORT ITEMS WITH CHART DATA ====================
        
        # Model Diagnosis Scenario:
        # We're analyzing an object detection model that classifies images into 4 categories:
        # Cat, Dog, Bird, and Fish. The model has been trained and we're diagnosing its performance.
        
        chart_configs = [
            {
                "type": AnalyticsReportItemType.CONFUSION_MATRIX,
                "name": "Confusion Matrix",
                "description": "Model prediction accuracy across all classes showing true vs predicted labels"
            },
            {
                "type": AnalyticsReportItemType.METRICS,
                "name": "Key Performance Metrics",
                "description": "Overall model performance indicators"
            },
            {
                "type": AnalyticsReportItemType.HORIZONTAL_BAR,
                "name": "Per-Class Precision",
                "description": "Precision score for each object class"
            },
            {
                "type": AnalyticsReportItemType.VERTICAL_BAR,
                "name": "Per-Class Recall",
                "description": "Recall score for each object class"
            },
            {
                "type": AnalyticsReportItemType.LINE_CHART,
                "name": "Precision-Recall Curve",
                "description": "Precision and recall trade-off at different confidence thresholds"
            },
            {
                "type": AnalyticsReportItemType.HISTOGRAM,
                "name": "Prediction Confidence Distribution",
                "description": "Distribution of model confidence scores across all predictions"
            },
            {
                "type": AnalyticsReportItemType.SCATTER_PLOT,
                "name": "Confidence vs IoU",
                "description": "Relationship between prediction confidence and Intersection over Union"
            },
            {
                "type": AnalyticsReportItemType.HEATMAP,
                "name": "Class Co-occurrence Matrix",
                "description": "Frequency of objects appearing together in images"
            },
            {
                "type": AnalyticsReportItemType.PIE,
                "name": "Class Distribution in Test Set",
                "description": "Proportion of each class in the test dataset"
            },
            {
                "type": AnalyticsReportItemType.TABLE,
                "name": "Detailed Performance by Class",
                "description": "Comprehensive metrics breakdown for each class"
            },
        ]
        
        print(f"\n[Step 13] Creating diagnosis report items for model performance analysis...")
        print(f"   üìä Scenario: Object Detection Model - 4 Classes (Cat, Dog, Bird, Fish)")
        try:
            for i, config in enumerate(chart_configs):
                chart_type = config["type"]
                # Create content folder for this chart
                folder_name = f"diagnosis_chart_{chart_type.value}_{int(time.time())}"
                try:
                    folder_content, upload_url = content_service.create_content(
                        key=folder_name,
                        content_type="FOLDER"
                    )
                    content_id = folder_content.id
                    print(f"   [{i+1}/{len(chart_configs)}] Creating {config['name']}...")
                except Exception as content_error:
                    print(f"   ‚ö†Ô∏è  Failed to create folder for {chart_type.value}: {content_error}")
                    continue
                
                # Generate chart data based on type
                chart_data = None
                try:
                    if chart_type == AnalyticsReportItemType.CONFUSION_MATRIX:
                        # Confusion matrix showing model predictions
                        # Good performance overall, but some confusion between Cat/Dog and Bird/Fish
                        chart_data = ChartDataFactory.create_confusion_matrix_chart(
                            true_axis_name="Ground Truth",
                            predicted_axis_name="Predicted Class",
                            data=[
                                # Cat predictions
                                ConfusionMatrixData(true_class="Cat", predicted_class="Cat", count=892),
                                ConfusionMatrixData(true_class="Cat", predicted_class="Dog", count=45),
                                ConfusionMatrixData(true_class="Cat", predicted_class="Bird", count=8),
                                ConfusionMatrixData(true_class="Cat", predicted_class="Fish", count=5),
                                # Dog predictions
                                ConfusionMatrixData(true_class="Dog", predicted_class="Cat", count=38),
                                ConfusionMatrixData(true_class="Dog", predicted_class="Dog", count=1047),
                                ConfusionMatrixData(true_class="Dog", predicted_class="Bird", count=12),
                                ConfusionMatrixData(true_class="Dog", predicted_class="Fish", count=3),
                                # Bird predictions
                                ConfusionMatrixData(true_class="Bird", predicted_class="Cat", count=6),
                                ConfusionMatrixData(true_class="Bird", predicted_class="Dog", count=9),
                                ConfusionMatrixData(true_class="Bird", predicted_class="Bird", count=745),
                                ConfusionMatrixData(true_class="Bird", predicted_class="Fish", count=40),
                                # Fish predictions
                                ConfusionMatrixData(true_class="Fish", predicted_class="Cat", count=4),
                                ConfusionMatrixData(true_class="Fish", predicted_class="Dog", count=7),
                                ConfusionMatrixData(true_class="Fish", predicted_class="Bird", count=35),
                                ConfusionMatrixData(true_class="Fish", predicted_class="Fish", count=654),
                            ]
                        )
                    
                    elif chart_type == AnalyticsReportItemType.METRICS:
                        # Key performance metrics for the model
                        chart_data = ChartDataFactory.create_metrics_chart(
                            metrics=[
                                MetricData(key="Overall Accuracy", value="91.3%"),
                                MetricData(key="Mean Average Precision (mAP)", value="0.876"),
                                MetricData(key="Mean IoU", value="0.782"),
                                MetricData(key="F1 Score", value="0.893"),
                                MetricData(key="Inference Speed", value="45.2 FPS"),
                                MetricData(key="Model Size", value="127 MB"),
                                MetricData(key="Total Test Images", value=4000),
                                MetricData(key="Total Predictions", value=4138),
                            ]
                        )
                    
                    elif chart_type == AnalyticsReportItemType.HORIZONTAL_BAR:
                        # Per-class precision showing which classes are most accurately predicted
                        chart_data = ChartDataFactory.create_horizontal_bar_chart(
                            y_axis_name="Class",
                            x_axis_name="Precision",
                            data=[
                                CategoryValueData(category="Dog", value=0.952),
                                CategoryValueData(category="Cat", value=0.940),
                                CategoryValueData(category="Bird", value=0.931),
                                CategoryValueData(category="Fish", value=0.933),
                            ],
                            data_ids=[
                                DataIdsIndex(index="Dog", data_ids=["dog_001", "dog_002", "dog_003"]),
                                DataIdsIndex(index="Cat", data_ids=["cat_001", "cat_002"]),
                                DataIdsIndex(index="Bird", data_ids=["bird_001", "bird_002"]),
                                DataIdsIndex(index="Fish", data_ids=["fish_001", "fish_002"]),
                            ]
                        )
                    
                    elif chart_type == AnalyticsReportItemType.VERTICAL_BAR:
                        # Per-class recall showing detection sensitivity
                        chart_data = ChartDataFactory.create_vertical_bar_chart(
                            x_axis_name="Class",
                            y_axis_name="Recall",
                            data=[
                                CategoryValueData(category="Cat", value=0.940),
                                CategoryValueData(category="Dog", value=0.951),
                                CategoryValueData(category="Bird", value=0.931),
                                CategoryValueData(category="Fish", value=0.933),
                            ]
                        )
                    
                    elif chart_type == AnalyticsReportItemType.LINE_CHART:
                        # Precision-Recall curve at different confidence thresholds
                        chart_data = ChartDataFactory.create_line_chart(
                            x_name="Confidence Threshold",
                            y_name="Score",
                            data=[
                                # Precision line
                                LineChartData(series="Precision", x=0.1, y=0.65),
                                LineChartData(series="Precision", x=0.2, y=0.72),
                                LineChartData(series="Precision", x=0.3, y=0.78),
                                LineChartData(series="Precision", x=0.4, y=0.84),
                                LineChartData(series="Precision", x=0.5, y=0.89),
                                LineChartData(series="Precision", x=0.6, y=0.92),
                                LineChartData(series="Precision", x=0.7, y=0.94),
                                LineChartData(series="Precision", x=0.8, y=0.96),
                                LineChartData(series="Precision", x=0.9, y=0.98),
                                # Recall line
                                LineChartData(series="Recall", x=0.1, y=0.98),
                                LineChartData(series="Recall", x=0.2, y=0.95),
                                LineChartData(series="Recall", x=0.3, y=0.92),
                                LineChartData(series="Recall", x=0.4, y=0.89),
                                LineChartData(series="Recall", x=0.5, y=0.85),
                                LineChartData(series="Recall", x=0.6, y=0.79),
                                LineChartData(series="Recall", x=0.7, y=0.72),
                                LineChartData(series="Recall", x=0.8, y=0.63),
                                LineChartData(series="Recall", x=0.9, y=0.48),
                                # F1 Score line
                                LineChartData(series="F1 Score", x=0.1, y=0.78),
                                LineChartData(series="F1 Score", x=0.2, y=0.82),
                                LineChartData(series="F1 Score", x=0.3, y=0.84),
                                LineChartData(series="F1 Score", x=0.4, y=0.86),
                                LineChartData(series="F1 Score", x=0.5, y=0.87),
                                LineChartData(series="F1 Score", x=0.6, y=0.85),
                                LineChartData(series="F1 Score", x=0.7, y=0.81),
                                LineChartData(series="F1 Score", x=0.8, y=0.76),
                                LineChartData(series="F1 Score", x=0.9, y=0.64),
                            ]
                        )
                    
                    elif chart_type == AnalyticsReportItemType.HISTOGRAM:
                        # Distribution of prediction confidence scores
                        chart_data = ChartDataFactory.create_histogram_chart(
                            bin_name="Confidence Score",
                            frequency_name="Number of Predictions",
                            data=[
                                BinFrequencyData(bin="0.0-0.1", frequency=12),
                                BinFrequencyData(bin="0.1-0.2", frequency=18),
                                BinFrequencyData(bin="0.2-0.3", frequency=45),
                                BinFrequencyData(bin="0.3-0.4", frequency=89),
                                BinFrequencyData(bin="0.4-0.5", frequency=156),
                                BinFrequencyData(bin="0.5-0.6", frequency=267),
                                BinFrequencyData(bin="0.6-0.7", frequency=412),
                                BinFrequencyData(bin="0.7-0.8", frequency=678),
                                BinFrequencyData(bin="0.8-0.9", frequency=1234),
                                BinFrequencyData(bin="0.9-1.0", frequency=1227),
                            ]
                        )
                    
                    elif chart_type == AnalyticsReportItemType.SCATTER_PLOT:
                        # Confidence vs IoU for all predictions
                        chart_data = ChartDataFactory.create_scatter_plot_chart(
                            x_name="Prediction Confidence",
                            y_name="IoU (Intersection over Union)",
                            data=[
                                # High confidence, high IoU (good predictions)
                                ScatterPlotData(x=0.95, y=0.87, category="Cat"),
                                ScatterPlotData(x=0.92, y=0.85, category="Dog"),
                                ScatterPlotData(x=0.89, y=0.82, category="Bird"),
                                ScatterPlotData(x=0.91, y=0.84, category="Fish"),
                                ScatterPlotData(x=0.94, y=0.88, category="Cat"),
                                ScatterPlotData(x=0.88, y=0.79, category="Dog"),
                                # Medium confidence, medium IoU
                                ScatterPlotData(x=0.75, y=0.68, category="Bird"),
                                ScatterPlotData(x=0.72, y=0.65, category="Fish"),
                                ScatterPlotData(x=0.68, y=0.61, category="Cat"),
                                ScatterPlotData(x=0.71, y=0.67, category="Dog"),
                                # Lower confidence, lower IoU
                                ScatterPlotData(x=0.55, y=0.48, category="Bird"),
                                ScatterPlotData(x=0.52, y=0.45, category="Fish"),
                                ScatterPlotData(x=0.58, y=0.51, category="Cat"),
                                ScatterPlotData(x=0.48, y=0.42, category="Dog"),
                                # Edge cases
                                ScatterPlotData(x=0.35, y=0.28, category="Bird"),
                                ScatterPlotData(x=0.42, y=0.35, category="Fish"),
                            ]
                        )
                    
                    elif chart_type == AnalyticsReportItemType.HEATMAP:
                        # Co-occurrence matrix showing which objects appear together
                        chart_data = ChartDataFactory.create_heatmap_chart(
                            y_axis_name="Primary Object",
                            x_axis_name="Secondary Object",
                            data=[
                                HeatmapData(y_category="Cat", x_category="Cat", value=125),
                                HeatmapData(y_category="Cat", x_category="Dog", value=78),
                                HeatmapData(y_category="Cat", x_category="Bird", value=45),
                                HeatmapData(y_category="Cat", x_category="Fish", value=12),
                                HeatmapData(y_category="Dog", x_category="Cat", value=82),
                                HeatmapData(y_category="Dog", x_category="Dog", value=156),
                                HeatmapData(y_category="Dog", x_category="Bird", value=38),
                                HeatmapData(y_category="Dog", x_category="Fish", value=9),
                                HeatmapData(y_category="Bird", x_category="Cat", value=42),
                                HeatmapData(y_category="Bird", x_category="Dog", value=35),
                                HeatmapData(y_category="Bird", x_category="Bird", value=98),
                                HeatmapData(y_category="Bird", x_category="Fish", value=28),
                                HeatmapData(y_category="Fish", x_category="Cat", value=15),
                                HeatmapData(y_category="Fish", x_category="Dog", value=11),
                                HeatmapData(y_category="Fish", x_category="Bird", value=31),
                                HeatmapData(y_category="Fish", x_category="Fish", value=67),
                            ]
                        )
                    
                    elif chart_type == AnalyticsReportItemType.PIE:
                        # Distribution of classes in test dataset
                        chart_data = ChartDataFactory.create_pie_chart(
                            category_name="Object Class",
                            value_name="Number of Images",
                            data=[
                                CategoryValueData(category="Dog", value=1100),
                                CategoryValueData(category="Cat", value=950),
                                CategoryValueData(category="Bird", value=800),
                                CategoryValueData(category="Fish", value=700),
                            ]
                        )
                    
                    elif chart_type == AnalyticsReportItemType.TABLE:
                        # Detailed performance metrics by class
                        chart_data = ChartDataFactory.create_table_chart(
                            headers=["Class", "Precision", "Recall", "F1-Score", "Support", "AP@0.5", "AP@0.75"],
                            rows=[
                                ["Cat", "0.940", "0.940", "0.940", "950", "0.952", "0.887"],
                                ["Dog", "0.952", "0.951", "0.952", "1100", "0.968", "0.912"],
                                ["Bird", "0.931", "0.931", "0.931", "800", "0.945", "0.865"],
                                ["Fish", "0.933", "0.933", "0.933", "700", "0.941", "0.879"],
                                ["Overall", "0.939", "0.939", "0.939", "3550", "0.952", "0.886"],
                            ]
                        )
                    
                    if chart_data:
                        # Upload reports.json
                        diagnosis_service.upload_reports_json(content_id, chart_data)
                        print(f"      ‚úÖ Uploaded reports.json")
                        
                        # Upload data_ids.json if exists
                        if chart_data.data_ids_json:
                            diagnosis_service.upload_data_ids_json(content_id, chart_data)
                            print(f"      ‚úÖ Uploaded data_ids.json")
                    
                except Exception as chart_error:
                    print(f"   ‚ö†Ô∏è  Failed to create/upload chart data for {chart_type.value}: {chart_error}")
                    continue
                
                # Create diagnosis report item
                test_report_name = config["name"]
                test_report_description = config["description"]
                
                try:
                    diagnosis_with_report = diagnosis_service.create_diagnosis_report_item(
                        dataset_id=DATASET_ID,
                        diagnosis_id=created_diagnosis_id,
                        name=test_report_name,
                        type=chart_type,
                        content_id=content_id,
                        description=test_report_description
                    )
                    
                    if diagnosis_with_report.diagnosis_report_items:
                        if isinstance(diagnosis_with_report.diagnosis_report_items, list):
                            for report in diagnosis_with_report.diagnosis_report_items:
                                if report.name == test_report_name:
                                    created_diagnosis_report_ids.append(report.id)
                                    print(f"   ‚úÖ Created diagnosis report {i+1}: {chart_type.value}")
                                    print(f"      Report ID: {report.id}")
                                    print(f"      Content ID: {content_id}")
                                    break
                        else:
                            created_diagnosis_report_ids.append(diagnosis_with_report.diagnosis_report_items.id)
                            print(f"   ‚úÖ Created diagnosis report {i+1}: {chart_type.value}")
                            print(f"      Report ID: {diagnosis_with_report.diagnosis_report_items.id}")
                            print(f"      Content ID: {content_id}")
                
                except Exception as report_error:
                    print(f"   ‚ö†Ô∏è  Failed to create diagnosis report for {chart_type.value}: {report_error}")
            
            print(f"‚úÖ Created {len(created_diagnosis_report_ids)} diagnosis report items with chart data")
            
        except Exception as e:
            print(f"‚ùå Failed to create diagnosis report items: {e}")
            import traceback
            traceback.print_exc()
            print(f"   Continuing with remaining tests...")
        
        # ==================== UPDATE DIAGNOSIS REPORT ITEMS ====================
        
        if len(created_diagnosis_report_ids) > 0:
            print(f"\n[Step 14] Updating diagnosis report items...")
            try:
                # Update the first report
                num_to_update = min(2, len(created_diagnosis_report_ids))
                updated_count = 0
                for i, report_id in enumerate(created_diagnosis_report_ids[:num_to_update]):
                    updated_report_name = f"updated_diagnosis_report_{int(time.time())}_{i}"
                    updated_report_description = f"Updated diagnosis report {i+1} description"
                    
                    try:
                        diagnosis_with_updated_report = diagnosis_service.update_diagnosis_report_item(
                            dataset_id=DATASET_ID,
                            diagnosis_id=created_diagnosis_id,
                            diagnosis_report_item_id=report_id,
                            name=updated_report_name,
                            description=updated_report_description
                        )
                        
                        print(f"   ‚úÖ Updated diagnosis report {i+1}/{num_to_update}:")
                        print(f"      Report ID: {report_id}")
                        print(f"      Updated Name: {updated_report_name}")
                        print(f"      Updated Description: {updated_report_description}")
                        updated_count += 1
                    except Exception as update_error:
                        print(f"   ‚ö†Ô∏è  Failed to update diagnosis report {i+1}: {update_error}")
                
                print(f"‚úÖ Updated {updated_count}/{num_to_update} diagnosis report items")
                
            except Exception as e:
                print(f"‚ùå Failed to update diagnosis report items: {e}")
                print(f"   Continuing with remaining tests...")
        else:
            print("\n[Step 14] Skipping diagnosis report update (no reports created)")
        
        # ==================== DELETE DIAGNOSIS REPORT ITEMS ====================
        
        if CLEANUP and len(created_diagnosis_report_ids) > 0:
            print(f"\n[Step 15] Deleting diagnosis report items...")
            try:
                deleted_count = 0
                # Delete all created diagnosis reports
                for i, report_id in enumerate(created_diagnosis_report_ids):
                    try:
                        diagnosis_after_report_delete = diagnosis_service.delete_diagnosis_report_item(
                            dataset_id=DATASET_ID,
                            diagnosis_id=created_diagnosis_id,
                            diagnosis_report_item_id=report_id
                        )
                        deleted_count += 1
                        print(f"   ‚úÖ Deleted diagnosis report {i+1}/{len(created_diagnosis_report_ids)}: {report_id}")
                    except Exception as delete_error:
                        print(f"   ‚ö†Ô∏è  Failed to delete diagnosis report {i+1}: {delete_error}")
                
                print(f"‚úÖ Deleted {deleted_count}/{len(created_diagnosis_report_ids)} diagnosis report items")
                
                # Verify reports were deleted
                try:
                    verification_diagnosis = diagnosis_service.get_diagnosis(
                        dataset_id=DATASET_ID,
                        diagnosis_id=created_diagnosis_id
                    )
                    
                    if verification_diagnosis.diagnosis_report_items:
                        if isinstance(verification_diagnosis.diagnosis_report_items, list):
                            remaining_count = len(verification_diagnosis.diagnosis_report_items)
                            print(f"   ‚ö†Ô∏è  {remaining_count} diagnosis reports still present after deletion")
                        else:
                            print(f"   ‚ö†Ô∏è  Diagnosis report still present after deletion")
                    else:
                        print(f"   ‚úÖ All diagnosis reports successfully deleted")
                except Exception as verify_error:
                    print(f"   ‚ö†Ô∏è  Could not verify deletion: {verify_error}")
                
            except Exception as e:
                print(f"‚ùå Failed to delete diagnosis report items: {e}")
                print(f"   Continuing with diagnosis verification...")
        else:
            if len(created_diagnosis_report_ids) > 0:
                print(f"\n[Step 15] Skipping diagnosis report deletion (CLEANUP=0)")
                print(f"   {len(created_diagnosis_report_ids)} diagnosis reports remain in the system")
            else:
                print("\n[Step 15] Skipping diagnosis report deletion (no reports created)")
        
        # ==================== VERIFY DIAGNOSIS STATE BEFORE DELETION ====================
        
        print("\n[Step 16] Verifying diagnosis state before deletion...")
        try:
            diagnosis_before_delete = diagnosis_service.get_diagnosis(
                dataset_id=DATASET_ID,
                diagnosis_id=created_diagnosis_id
            )
            
            print(f"‚úÖ Retrieved diagnosis state before deletion")
            print(f"   Diagnosis ID: {diagnosis_before_delete.id}")
            print(f"   Name: {diagnosis_before_delete.name}")
            print(f"   Status: {diagnosis_before_delete.status}")
            print(f"   Description: {diagnosis_before_delete.description}")
            print(f"   Score: {diagnosis_before_delete.score_key} = {diagnosis_before_delete.score_value} {diagnosis_before_delete.score_unit}")
            print(f"   Diagnosis Parameters: {diagnosis_before_delete.diagnosis_parameters}")
            print(f"   Data Counts: source={diagnosis_before_delete.source_data_count}, target={diagnosis_before_delete.target_data_count}, diagnosis={diagnosis_before_delete.diagnosis_data_count}")
            
        except Exception as e:
            print(f"‚ùå Failed to verify diagnosis state: {e}")
            pytest.fail(str(e))
        
        # ==================== DELETE DIAGNOSIS ====================
        
        if CLEANUP:
            print("\n[Step 17] Deleting the test diagnosis...")
            try:
                delete_result = diagnosis_service.delete_diagnosis(
                    dataset_id=DATASET_ID,
                    diagnosis_id=created_diagnosis_id
                )
                
                print(f"‚úÖ Diagnosis deletion executed successfully")
                print(f"   Delete Result: {delete_result}")
                
                assert delete_result == True, "Delete operation should return True"
                
            except Exception as e:
                print(f"‚ùå Failed to delete diagnosis: {e}")
                pytest.fail(str(e))
        else:
            print("\n[Step 17] Skipping diagnosis deletion (CLEANUP=0)")
            print(f"   Diagnosis remains in the system: {created_diagnosis_id}")
        
        # ==================== VERIFY DIAGNOSIS DELETION ====================
        
        if CLEANUP:
            print("\n[Step 18] Verifying diagnosis deletion...")
            try:
                deleted_diagnosis = diagnosis_service.get_diagnosis(
                    dataset_id=DATASET_ID,
                    diagnosis_id=created_diagnosis_id
                )
                
                if deleted_diagnosis is None:
                    print(f"‚úÖ Diagnosis successfully deleted (returns None)")
                else:
                    print(f"‚ö†Ô∏è  Diagnosis still exists after deletion:")
                    print(f"   Diagnosis ID: {deleted_diagnosis.id}")
                    print(f"   Name: {deleted_diagnosis.name}")
                    pytest.fail("Diagnosis should be deleted but still exists")
                    
            except Exception as e:
                # NotFoundError is expected when diagnosis is deleted
                from spb_onprem.exceptions import NotFoundError
                if isinstance(e, NotFoundError):
                    print(f"‚úÖ Diagnosis successfully deleted (NotFoundError raised as expected)")
                else:
                    print(f"‚ùå Unexpected error while verifying deletion: {e}")
                    pytest.fail(str(e))
        
        # ==================== FINAL SUCCESS MESSAGE ====================
        
        print("\n" + "=" * 80)
        print("Diagnosis Service Complete Lifecycle Workflow Test Passed Successfully! üéâ")
        print("=" * 80)
        print("\nTest Summary:")
        print(f"  ‚úì Dataset ID: {DATASET_ID}")
        print(f"  ‚úì Created diagnosis: {created_diagnosis_id}")
        print(f"  ‚úì Diagnosis name: {test_diagnosis_name}")
        print(f"  ‚úì Tested all new fields: contents, source/target slice IDs, data counts")
        print(f"  ‚úì Created {len(created_diagnosis_report_ids)} diagnosis report items with chart data")
        print(f"  ‚úì Tested CONFUSION_MATRIX chart type (new in AnalyticsReportItemType)")
        if CLEANUP:
            print(f"  ‚úì Cleaned up all diagnosis reports and diagnosis")
        else:
            print(f"  ‚ÑπÔ∏è  Diagnosis and reports remain in system (CLEANUP=0)")
            print(f"     Diagnosis ID: {created_diagnosis_id}")
        print("\nTo run without cleanup:")
        print("  CLEANUP=0 RUN_DIAGNOSIS_WORKFLOW_TESTS=1 python -m pytest tests/diagnoses/test_workflow.py::test_diagnosis_lifecycle_workflow")
        print("=" * 80)
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå Workflow test failed with exception: {e}")
        import traceback
        traceback.print_exc()
        
        if CLEANUP:
            print(f"\n‚ö†Ô∏è  Attempting cleanup...")
            # Cleanup: try to delete the diagnosis if it was created
            if created_diagnosis_id:
                try:
                    print(f"   Attempting to delete diagnosis: {created_diagnosis_id}")
                    diagnosis_service.delete_diagnosis(
                        dataset_id=DATASET_ID,
                        diagnosis_id=created_diagnosis_id
                    )
                    print(f"   ‚úÖ Cleanup successful - diagnosis deleted")
                except Exception as cleanup_error:
                    print(f"   ‚ö†Ô∏è  Cleanup failed: {cleanup_error}")
                    print(f"   ‚ö†Ô∏è  Please manually delete diagnosis: {created_diagnosis_id}")
        else:
            if created_diagnosis_id:
                print(f"\n‚ö†Ô∏è  Diagnosis remains in system (CLEANUP=0): {created_diagnosis_id}")
        
        pytest.fail(str(e))


if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("Running Diagnosis Service Workflow Tests")
    print("=" * 80)
    print("\nTest: Complete Lifecycle Workflow")
    test_diagnosis_lifecycle_workflow()
    
    print("\n" + "=" * 80)
    print("All Diagnosis Workflow Tests Completed Successfully! üéâ")
    print("=" * 80)
